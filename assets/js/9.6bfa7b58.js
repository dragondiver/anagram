(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{406:function(t,e,a){"use strict";a.r(e);var r=a(56),s=Object(r.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"finding-the-best-solution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#finding-the-best-solution"}},[t._v("#")]),t._v(" Finding the Best Solution")]),t._v(" "),a("h2",{attrs:{id:"the-problem"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#the-problem"}},[t._v("#")]),t._v(" The Problem")]),t._v(" "),a("p",[t._v("Given an input file which contains one word per line, as an output construct a list of all anagrams from that input file. Print those words to the console, where all words that are an anagram should each other should be on the same line.")]),t._v(" "),a("h2",{attrs:{id:"main-topics-in-approach"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#main-topics-in-approach"}},[t._v("#")]),t._v(" Main Topics in Approach")]),t._v(" "),a("h4",{attrs:{id:"where-to-store-the-anagrams-collection"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#where-to-store-the-anagrams-collection"}},[t._v("#")]),t._v(" Where to store the anagrams collection")]),t._v(" "),a("p",[t._v("As long as we do have enough memory, the obvious solution is a map, with larger amounts of words, an appropriate solution would be a database of some kind, that is able to handle the amount of data.")]),t._v(" "),a("h4",{attrs:{id:"how-to-identify-an-anagram"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#how-to-identify-an-anagram"}},[t._v("#")]),t._v(" How to identify an anagram?")]),t._v(" "),a("p",[t._v("Two words are defined as anagrams if they do share the same letters, but are in a different order (i.e. the English words race and care are anagrams).\nImmediate solutions would be to sort the characters and use the string of sorted char as key.")]),t._v(" "),a("p",[t._v("To be able to identify a colection of anagrams, we need a key. But because sorting is expensive, an other solution could be to calculate the hashcode of a word by calculating the product of each character mapping prime. This works since the product of 2 primes is unique from the products of any other primes. I tested the performance in the exploration sub project.")]),t._v(" "),a("h4",{attrs:{id:"is-writing-to-system-out-affecting-my-performance"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#is-writing-to-system-out-affecting-my-performance"}},[t._v("#")]),t._v(" Is writing to System.out affecting my performance?")]),t._v(" "),a("p",[t._v("Another problem might occur in the output speed.")]),t._v(" "),a("h2",{attrs:{id:"testing-the-performane-of-output"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#testing-the-performane-of-output"}},[t._v("#")]),t._v(" Testing the performane of output")]),t._v(" "),a("p",[t._v("In the "),a("strong",[t._v("exploration")]),t._v(" module in the package "),a("em",[t._v("com.fhuber.schwarz.output")]),t._v("* i compare writing with System.out and with a BufferedWriter.\nYou can run it from root folder with ./scripts/runOutput.sh. In Folder results you can find the results.")]),t._v(" "),a("table",[a("thead",[a("tr",[a("th",[t._v("Method")]),t._v(" "),a("th",{staticStyle:{"text-align":"right"}},[t._v("Time")]),t._v(" "),a("th",{staticStyle:{"text-align":"right"}},[t._v("Number of Words")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("System.out.println()")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("536309458 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("235886")])]),t._v(" "),a("tr",[a("td",[t._v("System.out.println()")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("42281179833 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("11794300")])]),t._v(" "),a("tr",[a("td",[t._v("BufferedWriter")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("288006959 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("235886")])]),t._v(" "),a("tr",[a("td",[t._v("BufferedWriter")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("18766103417 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("11794300")])]),t._v(" "),a("tr",[a("td",[t._v("FileWriter")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("395819250 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("235886")])]),t._v(" "),a("tr",[a("td",[t._v("FileWriter")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("8038425791 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("11794300")])])])]),t._v(" "),a("p",[t._v("You can see, it is worthwhile to think about that too. For large files the advantage of BufferedWriter seems to shrink.\nIt is also important to consider the environment, e.g. whether the terminal displays the data, or it is logged into file.\nTo reduce side effects you might want to use")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[t._v("./scripts/runOutput.sh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("/dev/null\n")])])]),a("p",[t._v("For the moment i will stick with writing to FileWriter directly, because i do have a fast Apple Macbook Air M1 ðŸ˜‰")]),t._v(" "),a("h2",{attrs:{id:"measuring-the-performance-of-the-hashkey-creation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#measuring-the-performance-of-the-hashkey-creation"}},[t._v("#")]),t._v(" Measuring the performance of the hashKey creation")]),t._v(" "),a("p",[t._v("To make sure to find the anagrams, i added code to extract special characters and blanks from the lines.")]),t._v(" "),a("div",{staticClass:"language-java extra-class"},[a("pre",{pre:!0,attrs:{class:"language-java"}},[a("code",[t._v("word"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("toLowerCase")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("replaceAll")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[^\\\\p{IsAlphabetic}\\\\p{IsDigit}]"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("This code was not examined for performance to reduce the problem.")]),t._v(" "),a("p",[t._v("In the "),a("strong",[t._v("exploration")]),t._v(" module in the package "),a("em",[t._v("com.fhuber.schwarz.hashkey")]),t._v("* i compare generating with sorted char array against "),a("em",[t._v("prime_mod")]),t._v(".\nYou can run it from root folder with ./scripts/runhash.sh. In Folder results you can find the results.")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[t._v("./scripts/runHash.sh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("/dev/null\n")])])]),a("table",[a("thead",[a("tr",[a("th",[t._v("Method")]),t._v(" "),a("th",{staticStyle:{"text-align":"right"}},[t._v("Time")]),t._v(" "),a("th",{staticStyle:{"text-align":"right"}},[t._v("Number of Words")])])]),t._v(" "),a("tbody",[a("tr",[a("td",[t._v("HashKeyFromPrime")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("420534792 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("235886")])]),t._v(" "),a("tr",[a("td",[t._v("HashKeyFromPrime")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("8186032875 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("11794300")])]),t._v(" "),a("tr",[a("td",[t._v("HashKeyFromSort")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("408632417 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("235886")])]),t._v(" "),a("tr",[a("td",[t._v("HashKeyFromSort")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("8063013458 ms")]),t._v(" "),a("td",{staticStyle:{"text-align":"right"}},[t._v("11794300")])])])]),t._v(" "),a("p",[t._v("I will choose the method "),a("strong",[t._v("HashKeyFromSort")]),t._v(" in implementation")]),t._v(" "),a("h2",{attrs:{id:"testing-the-perfomance-of-the-map"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#testing-the-perfomance-of-the-map"}},[t._v("#")]),t._v(" Testing the perfomance of the Map")]),t._v(" "),a("p",[t._v("In the "),a("strong",[t._v("exploration")]),t._v(" module in the package "),a("em",[t._v("com.fhuber.schwarz.map")]),t._v("* i test the performance against HashMap and ConcurrentHashMap\nYou can run it from root folder with ./scripts/runMap.sh. In Folder results you can find the results.")]),t._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[t._v("./scripts/runMap.sh "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("/dev/null\n")])])]),a("p",[t._v("HashMap is not threadsafe, so it cannot execute in parallel. But as reading from File should not easily be parallelized it will do.\nStill the performance of ConcurrentHashMap is comparable, so we might just stick with it as well.\nUsing an in Memory HashMap stops loading at 57000000 words. So this is the place, where we should start working with a DB.")])])}),[],!1,null,null,null);e.default=s.exports}}]);